{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTRAINEMENT D'UN MODELE  D'EMBEDDING (POUR LA CLASSIFICATION DES TEXTES):\n",
    "-   on va partir d'une description textuelle d'une publication et par la suite faire une classification sur le dégré de succés de la vidéo en question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence-transformers\n",
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Sentence Transformers models work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/envs/challenge/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "\n",
    "# #Mean Pooling - Take attention mask into account for correct averaging\n",
    "# def mean_pooling(model_output, attention_mask):\n",
    "#     token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "#     input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "## Join steps 1 and 2 using the modules argument\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARATION OF THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Load the dataset\n",
    "\n",
    "train_data=pd.read_csv(\"../dataset/train_val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valeur minimale de la colonne views:  0\n",
      "valeur maximale de la colonne views:  190150188\n",
      "moyenne des valeurs de la colonne views:  562777.6693579641\n",
      "médiane des valeurs de la colonne views:  29294.5\n"
     ]
    }
   ],
   "source": [
    "#valeur maximale et minimale des valeurs de la colonne views\n",
    "print(\"valeur minimale de la colonne views: \",train_data[\"views\"].min())\n",
    "print(\"valeur maximale de la colonne views: \",train_data[\"views\"].max())\n",
    "print(\"moyenne des valeurs de la colonne views: \",train_data[\"views\"].mean())\n",
    "print(\"médiane des valeurs de la colonne views: \",train_data[\"views\"].median())\n",
    "\n",
    "#segementation des valeurs en 10 classes en fonction du succès de la vidéo\n",
    "bins = [0, 100, 1_000, 5_000, 10_000, 30_000,\n",
    "        100_000, 1_000_000, 5_000_000, 100_000_000, 190_000_000]\n",
    "\n",
    "labels = [\"Double-digit views\", \"Triple-digit views\", \"1 K Views Club\",\n",
    "          \"5 K Views Club\", \"10 K Views Club\", \"30 K + Zone\",\n",
    "          \"100 K Club\", \"1 M Club\", \"Multi-Million\", \"100 M Club\"]\n",
    "\n",
    "train_data[\"success\"] = pd.cut(train_data[\"views\"], bins=bins, labels=labels,\n",
    "                      right=True, include_lowest=True)\n",
    "\n",
    "#suppression des lignes avec des valeurs manquantes\n",
    "train_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>channel</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>views</th>\n",
       "      <th>year</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>--2s6hjGrm4</td>\n",
       "      <td>UC-1rx8j9Ggp8mp4uD0ZdEIA</td>\n",
       "      <td>CGI &amp; VFX Breakdowns: \"Warzone\" - by Ramesh Th...</td>\n",
       "      <td>2020-12-15 05:00:01+00:00</td>\n",
       "      <td>Check out this revealing VFX Breakdown \"Warzon...</td>\n",
       "      <td>12299</td>\n",
       "      <td>2020</td>\n",
       "      <td>10 K Views Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>--DnfroyKQ8</td>\n",
       "      <td>UC-1rx8j9Ggp8mp4uD0ZdEIA</td>\n",
       "      <td>A Sci-Fi Short Film: \"Exit\" - by Ng King Kwan ...</td>\n",
       "      <td>2020-07-01 16:00:00+00:00</td>\n",
       "      <td>TheCGBros Presents \"Exit\" by Ng King Kwan - Th...</td>\n",
       "      <td>7494</td>\n",
       "      <td>2020</td>\n",
       "      <td>5 K Views Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>--aiU7VQKEw</td>\n",
       "      <td>UC-1rx8j9Ggp8mp4uD0ZdEIA</td>\n",
       "      <td>CGI 3D Animated Short: \"Lost Love\" - by Akash ...</td>\n",
       "      <td>2019-02-18 20:30:00+00:00</td>\n",
       "      <td>TheCGBros Presents \"Lost Love\" by Akash Manack...</td>\n",
       "      <td>11831</td>\n",
       "      <td>2019</td>\n",
       "      <td>10 K Views Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>-0SrlZAvSVM</td>\n",
       "      <td>UCW6NyJ6oFLPTnx7iGRZXDDg</td>\n",
       "      <td>Jo Goes Hunting - Careful | Animated music vid...</td>\n",
       "      <td>2020-03-10 14:30:01+00:00</td>\n",
       "      <td>On the borderless map of a magical planet, lit...</td>\n",
       "      <td>2248</td>\n",
       "      <td>2020</td>\n",
       "      <td>1 K Views Club</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>-13Y2Pe7kFs</td>\n",
       "      <td>UC-1rx8j9Ggp8mp4uD0ZdEIA</td>\n",
       "      <td>CGI VFX Breakdown: \"Logan (Wolverine): Digital...</td>\n",
       "      <td>2017-09-20 20:13:52+00:00</td>\n",
       "      <td>Check out this outstanding behind-the-scenes l...</td>\n",
       "      <td>113806</td>\n",
       "      <td>2017</td>\n",
       "      <td>100 K Club</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           id                   channel  \\\n",
       "0           0  --2s6hjGrm4  UC-1rx8j9Ggp8mp4uD0ZdEIA   \n",
       "1           1  --DnfroyKQ8  UC-1rx8j9Ggp8mp4uD0ZdEIA   \n",
       "2           2  --aiU7VQKEw  UC-1rx8j9Ggp8mp4uD0ZdEIA   \n",
       "3           6  -0SrlZAvSVM  UCW6NyJ6oFLPTnx7iGRZXDDg   \n",
       "4          10  -13Y2Pe7kFs  UC-1rx8j9Ggp8mp4uD0ZdEIA   \n",
       "\n",
       "                                               title  \\\n",
       "0  CGI & VFX Breakdowns: \"Warzone\" - by Ramesh Th...   \n",
       "1  A Sci-Fi Short Film: \"Exit\" - by Ng King Kwan ...   \n",
       "2  CGI 3D Animated Short: \"Lost Love\" - by Akash ...   \n",
       "3  Jo Goes Hunting - Careful | Animated music vid...   \n",
       "4  CGI VFX Breakdown: \"Logan (Wolverine): Digital...   \n",
       "\n",
       "                        date  \\\n",
       "0  2020-12-15 05:00:01+00:00   \n",
       "1  2020-07-01 16:00:00+00:00   \n",
       "2  2019-02-18 20:30:00+00:00   \n",
       "3  2020-03-10 14:30:01+00:00   \n",
       "4  2017-09-20 20:13:52+00:00   \n",
       "\n",
       "                                         description   views  year  \\\n",
       "0  Check out this revealing VFX Breakdown \"Warzon...   12299  2020   \n",
       "1  TheCGBros Presents \"Exit\" by Ng King Kwan - Th...    7494  2020   \n",
       "2  TheCGBros Presents \"Lost Love\" by Akash Manack...   11831  2019   \n",
       "3  On the borderless map of a magical planet, lit...    2248  2020   \n",
       "4  Check out this outstanding behind-the-scenes l...  113806  2017   \n",
       "\n",
       "           success  \n",
       "0  10 K Views Club  \n",
       "1   5 K Views Club  \n",
       "2  10 K Views Club  \n",
       "3   1 K Views Club  \n",
       "4       100 K Club  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# construction du dataset\n",
    "dataset_frame=train_data[[\"title\",\"success\"]]\n",
    "\n",
    "dataset=Dataset.from_pandas(dataset_frame)\n",
    "\n",
    "#construction du dataset d'entrainement et de test\n",
    "split=dataset.train_test_split(test_size=0.2,seed=42)\n",
    "train_dataset=split[\"train\"]\n",
    "test_dataset=split[\"test\"]\n",
    "\n",
    "\n",
    "#construction du dataset d'entrainement et de validation\n",
    "split=train_dataset.train_test_split(test_size=0.2,seed=42)\n",
    "final_train_dataset=split[\"train\"]\n",
    "val_dataset=split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'success', '__index_level_0__'],\n",
      "    num_rows: 9712\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'success', '__index_level_0__'],\n",
      "    num_rows: 2428\n",
      "})\n",
      "Dataset({\n",
      "    features: ['title', 'success', '__index_level_0__'],\n",
      "    num_rows: 3035\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(final_train_dataset)\n",
    "print(val_dataset)\n",
    "print(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will start by the constractive learning : \n",
    "This approach help to train a semantic embedding model,so that texts in the same class are “close” in embedding space, and texts from different classes are “far” apart, then you use triplet loss or similar. \\\n",
    "Each triplet:\n",
    "\n",
    "- Anchor: A sample from class X.\n",
    "- Positive: Another sample from (the same) class X.\n",
    "- Negative: A sample from a different class Y (Y ≠ X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triplets: 9712\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample, losses\n",
    "import random\n",
    "from collections import defaultdict\n",
    "#convert labels to index\n",
    "label_to_indices=defaultdict(list)\n",
    "\n",
    "for i,row in enumerate(final_train_dataset):\n",
    "    label_to_indices[row[\"success\"]].append(i)\n",
    "\n",
    "train_examples=[]\n",
    "for anchor_idx, anchor in enumerate(final_train_dataset):\n",
    "    anchor_label = anchor[\"success\"]\n",
    "    anchor_desc = anchor[\"title\"]\n",
    "\n",
    "    #candidat positif\n",
    "    positive_idx=random.choice(label_to_indices[anchor_label]) if len(label_to_indices[anchor_label])>1 else anchor_idx\n",
    "    positive_desc=final_train_dataset[positive_idx][\"title\"]\n",
    "\n",
    "    #candidat négatif\n",
    "    negative_idx=random.choice(label_to_indices[anchor_label]) if len(label_to_indices[anchor_label])>1 else anchor_idx\n",
    "    negative_desc=final_train_dataset[negative_idx][\"title\"]\n",
    "\n",
    "    train_examples.append(InputExample(texts=[anchor_desc,positive_desc,negative_desc]))\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Number of triplets: {len(train_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<InputExample> label: 0, texts: CGI 3D Animated Short: \"Lost Love\" - by Akash Manackchand | TheCGBros; CGI VFX Animated TVC : \"SIDI ALI: Dancing Robot\" by Digital Golem; CGI 3D Animated Shorts : \"Milezim\" - by ESMA\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap our training dataset into a Pytorch `Dataloader` to shuffle examples and get batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE BEFORE TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "similarity between the two titles:  tensor([[0.3063]], device='cuda:0')\n",
      " the label of the first title is:  10 K Views Club\n",
      " the label of the second title is:  5 K Views Club\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3063]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "def inference(model, list_title_dict):\n",
    "    \"\"\"\n",
    "    list_title_dict=[\n",
    "    {\n",
    "    \"title\":\"title 1\",\n",
    "    \"success\":\"success 1\"\n",
    "    },\n",
    "    {\n",
    "    \"title\":\"title 2\",\n",
    "    \"success\":\"success 2\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    list_title=[value[\"title\"] for value in list_title_dict]\n",
    "    embedding=model.encode(list_title , convert_to_tensor=True)\n",
    "    print(embedding.shape)\n",
    "    similarity=cos_sim(embedding[0],embedding[1])\n",
    "    print(\"similarity between the two titles: \",similarity)\n",
    "    print(\" the label of the first title is: \",list_title_dict[0][\"success\"])\n",
    "    print(\" the label of the second title is: \",list_title_dict[1][\"success\"])\n",
    "    return similarity\n",
    "\n",
    "list_title_dict=[\n",
    "    {\n",
    "        \"title\":train_data[\"title\"][0],\n",
    "        \"success\":train_data[\"success\"][0]\n",
    "    },\n",
    "    {\n",
    "        \"title\":train_data[\"title\"][1],\n",
    "        \"success\":train_data[\"success\"][1]\n",
    "    }\n",
    "]\n",
    "inference(model,list_title_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for training a Sentence Transformers model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import losses\n",
    "\n",
    "train_loss = losses.TripletLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdehayemkenfouo\u001b[0m (\u001b[33mdehayemkenfouo-st\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/users/eleves-b/2023/sylvain.dehayem-kenfouo/projet_final_modal/models/wandb/run-20250505_225804-exfiiwun</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dehayemkenfouo-st/sentence-transformers/runs/exfiiwun' target=\"_blank\">checkpoints/model</a></strong> to <a href='https://wandb.ai/dehayemkenfouo-st/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dehayemkenfouo-st/sentence-transformers' target=\"_blank\">https://wandb.ai/dehayemkenfouo-st/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dehayemkenfouo-st/sentence-transformers/runs/exfiiwun' target=\"_blank\">https://wandb.ai/dehayemkenfouo-st/sentence-transformers/runs/exfiiwun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30350' max='30350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30350/30350 53:06, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.919600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.807700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.661800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.567800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.532000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.522600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.483500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.433600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.404100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.340200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.330600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.301200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.289700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import LoggingHandler, SentenceTransformer, losses\n",
    "import logging , os \n",
    "from datasets import Dataset\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s -   %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[LoggingHandler()]\n",
    ")\n",
    "logger=logging.getLogger(__name__)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1) #10% of train data\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 274.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n",
      "similarity between the two titles:  tensor([[1.0000]], device='cuda:0')\n",
      " the label of the first title is:  30 K + Zone\n",
      " the label of the second title is:  100 K Club\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_title_dict=[\n",
    "    {\n",
    "        \"title\":train_data[\"title\"][30],\n",
    "        \"success\":train_data[\"success\"][30]\n",
    "    },\n",
    "    {\n",
    "        \"title\":train_data[\"title\"][10],\n",
    "        \"success\":train_data[\"success\"][10]\n",
    "    }\n",
    "]\n",
    "inference(model,list_title_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papa Aap Sunn Rahe Hai Na? | Heart Touching Short Film Hindi | @SocialFootage\n",
      "1 K Views Club\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"title\"][6])\n",
    "print(train_dataset[\"success\"][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGI 3D Animated Branding Vignettes: \"Verizon Animations\" - by AssemblyLTD\n",
      "5 K Views Club\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"title\"][0])\n",
    "print(train_dataset[\"success\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): Traceback (most recent call last):\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/bin/huggingface-cli\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/commands/huggingface_cli.py\"\u001b[0m, line \u001b[35m57\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    \u001b[31mservice.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/commands/user.py\"\u001b[0m, line \u001b[35m153\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mlogin\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "    \u001b[31m~~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31mtoken=self.args.token,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "        \u001b[1;31madd_to_git_credential=self.args.add_to_git_credential,\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\"\u001b[0m, line \u001b[35m101\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\"\u001b[0m, line \u001b[35m31\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/_login.py\"\u001b[0m, line \u001b[35m130\u001b[0m, in \u001b[35mlogin\u001b[0m\n",
      "    \u001b[31minterpreter_login\u001b[0m\u001b[1;31m(new_session=new_session)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\"\u001b[0m, line \u001b[35m101\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/utils/_deprecation.py\"\u001b[0m, line \u001b[35m31\u001b[0m, in \u001b[35minner_f\u001b[0m\n",
      "    return f(*args, **kwargs)\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/site-packages/huggingface_hub/_login.py\"\u001b[0m, line \u001b[35m287\u001b[0m, in \u001b[35minterpreter_login\u001b[0m\n",
      "    token = getpass(\"Enter your token (input will not be visible): \")\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/getpass.py\"\u001b[0m, line \u001b[35m76\u001b[0m, in \u001b[35munix_getpass\u001b[0m\n",
      "    passwd = _raw_input(prompt, stream, input=input)\n",
      "  File \u001b[35m\"/users/eleves-b/2023/sylvain.dehayem-kenfouo/miniconda/lib/python3.13/getpass.py\"\u001b[0m, line \u001b[35m146\u001b[0m, in \u001b[35m_raw_input\u001b[0m\n",
      "    line = input.readline()\n",
      "  File \u001b[35m\"<frozen codecs>\"\u001b[0m, line \u001b[35m322\u001b[0m, in \u001b[35mdecode\u001b[0m\n",
      "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-05 23:51:13,040 -   Save model to /tmp/tmpo6ph2aqm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 90.9M/90.9M [00:05<00:00, 16.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Syldehayem/all-MiniLM-L6-v2_embedder_train/commit/252863ccc854c62c3fd77ff712cd59e976eba7bb'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\n",
    "    \"all-MiniLM-L6-v2_embedder_train\", \n",
    "    # organization=\"embedding-data\",\n",
    "    # train_datasets=[\"embedding-data/QQP_triplets\"],\n",
    "    exist_ok=True, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (challenge)",
   "language": "python",
   "name": "challenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
